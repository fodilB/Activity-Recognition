{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Flatten, Reshape, Dropout, Activation\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(66) # for reproductibility results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 7 7 7]\n"
     ]
    }
   ],
   "source": [
    "#Loading and Preparing the data\n",
    "data = pickle.load(open(\"clean_data.p\" , \"rb\"))\n",
    "\n",
    "data_frame = pd.DataFrame()\n",
    "\n",
    "#loading the file data and store them in data_frame\n",
    "for k in range(1,16):\n",
    "    index = 'nÂ° '+str(k)\n",
    "    data_frame = data_frame.append(pd.DataFrame(data[index]), ignore_index=True)\n",
    "\n",
    "#getting the target     \n",
    "y = data_frame['class'].values\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            x     y     z\n",
      "0        1502  2215  2153\n",
      "1        1667  2072  2047\n",
      "2        1611  1957  1906\n",
      "3        1601  1939  1831\n",
      "4        1643  1965  1879\n",
      "5        1604  1959  1921\n",
      "6        1640  1829  1940\n",
      "7        1607  1910  1910\n",
      "8        1546  2045  1910\n",
      "9        1529  2049  1972\n",
      "10       1637  1978  1945\n",
      "11       1596  2046  1866\n",
      "12       1590  2006  1978\n",
      "13       1601  1966  1957\n",
      "14       1542  2003  1959\n",
      "15       1598  2027  1941\n",
      "16       1511  2258  1983\n",
      "17       1555  1980  2023\n",
      "18       1508  2468  1934\n",
      "19       1580  1697  2005\n",
      "20       1627  2073  1992\n",
      "21       1592  2130  2063\n",
      "22       1634  2088  1991\n",
      "23       1638  2102  1916\n",
      "24       1593  2123  1948\n",
      "25       1542  2133  2034\n",
      "26       1601  2015  2042\n",
      "27       1613  1938  1936\n",
      "28       1644  1974  2000\n",
      "29       1642  1933  2046\n",
      "...       ...   ...   ...\n",
      "1923147  2039  2522  1999\n",
      "1923148  2042  2537  2001\n",
      "1923149  2057  2534  1987\n",
      "1923150  2036  2557  2005\n",
      "1923151  2048  2539  2003\n",
      "1923152  2042  2519  2005\n",
      "1923153  2043  2522  2006\n",
      "1923154  2033  2483  1994\n",
      "1923155  2013  2479  1988\n",
      "1923156  2011  2498  2005\n",
      "1923157  2024  2503  1998\n",
      "1923158  2033  2534  2003\n",
      "1923159  2055  2599  2013\n",
      "1923160  2079  2597  2000\n",
      "1923161  2055  2539  1982\n",
      "1923162  2037  2533  1995\n",
      "1923163  2033  2538  1996\n",
      "1923164  2041  2513  1991\n",
      "1923165  2049  2494  1993\n",
      "1923166  2044  2545  2006\n",
      "1923167  2050  2562  2009\n",
      "1923168  2067  2535  2002\n",
      "1923169  2059  2538  1997\n",
      "1923170  2057  2516  1983\n",
      "1923171  2046  2529  1990\n",
      "1923172  2050  2523  1991\n",
      "1923173  2043  2539  1990\n",
      "1923174  2036  2508  1976\n",
      "1923175  2025  2520  1991\n",
      "1923176  2009  2546  2007\n",
      "\n",
      "[1923177 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#delete the class column from the data frame\n",
    "del data_frame['class']\n",
    "\n",
    "print(data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **<span style=\"color:red;\">RobustScaler</span>** uses a similar method to the **Min-Max scaler** but it instead uses the **interquartile range**,\n",
    "rathar than the **min-max**, so that it is robust to **outliers**. Therefore it follows the formula:\n",
    "\n",
    "<span style=\"color:blue;\">RobustScaler</span> = $\\frac{x_{i}-Q_{1}(x)}{Q_{3}(x)-Q_{1}(x)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fodil/miniconda3/envs/deep-learning/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/fodil/miniconda3/envs/deep-learning/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "robust_scaled_df = scaler.fit_transform(data_frame)\n",
    "robust_scaled_df = pd.DataFrame(robust_scaled_df, columns=['x', 'y', 'z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.36029703 -1.66947853  1.93152004]\n",
      " [-2.87836023 -3.09647188  0.80972225]\n",
      " [-3.3813206  -4.24405394 -0.68248047]\n",
      " [-3.47113495 -4.42367548 -1.47620532]\n",
      " [-3.09391468 -4.16422214 -0.96822141]\n",
      " [-3.44419065 -4.22409599 -0.5237355 ]\n",
      " [-3.12085898 -5.52136267 -0.32265854]\n",
      " [-3.41724634 -4.71306574 -0.64014848]\n",
      " [-3.96511388 -3.36590419 -0.64014848]\n",
      " [-4.11779828 -3.32598829  0.0159974 ]]\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "[[-1.00124029 -0.17263237  1.6351961 ]\n",
      " [ 1.02856404 -0.64164416 -0.12158157]\n",
      " [-1.62095932 -0.45204365  0.36523634]\n",
      " [-1.15392469 -0.48198057  0.21707436]\n",
      " [ 1.28004422  0.84522303 -1.26454536]\n",
      " [ 0.67828807 -0.41212775  0.70389227]\n",
      " [ 1.11837839 -0.85120263  0.89438624]\n",
      " [-0.02226387  0.00698917  0.36523634]\n",
      " [ 1.29800709 -0.31233801 -0.12158157]\n",
      " [-1.03716603 -0.02294775  0.97905022]]\n",
      "[1 3 2 4 6 7 3 3 4 1]\n"
     ]
    }
   ],
   "source": [
    "X = robust_scaled_df.values\n",
    "print(X[:10])\n",
    "print(y[:10])\n",
    "\n",
    "X2, y2 = shuffle(X, y, random_state=66)\n",
    "print(X2[:10])\n",
    "print(y2[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X2, y2, test_size=0.2,  stratify=y, shuffle=True, random_state=66)\n",
    "\n",
    "#Resizing the inputs to 3D dimension inputs (format needed by ) \n",
    "x_train = X_train.reshape(-1,1,3)\n",
    "x_test  = X_test.reshape(-1,1,3)\n",
    "\n",
    "#-1 : To start from 0 , cause one hot encoding of keras start from 0\n",
    "y_train = y_train-1\n",
    "y_test = y_test-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 1, 128)            67584     \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 1, 128)            131584    \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 200,071\n",
      "Trainable params: 200,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1230832 samples, validate on 307709 samples\n",
      "Epoch 1/15\n",
      "1230832/1230832 [==============================] - 276s 224us/step - loss: 1.0279 - acc: 0.6564 - val_loss: 0.9198 - val_acc: 0.6980\n",
      "Epoch 2/15\n",
      "1230832/1230832 [==============================] - 250s 203us/step - loss: 0.8967 - acc: 0.7046 - val_loss: 0.8784 - val_acc: 0.7136\n",
      "Epoch 3/15\n",
      "1230832/1230832 [==============================] - 238s 194us/step - loss: 0.8651 - acc: 0.7146 - val_loss: 0.8584 - val_acc: 0.7164\n",
      "Epoch 4/15\n",
      "1230832/1230832 [==============================] - 235s 191us/step - loss: 0.8484 - acc: 0.7196 - val_loss: 0.8432 - val_acc: 0.7229\n",
      "Epoch 5/15\n",
      " 852864/1230832 [===================>..........] - ETA: 1:08 - loss: 0.8403 - acc: 0.7220"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e9ddbc7c9519>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m model.fit(x_train, yy_train,\n\u001b[1;32m     46\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mNB_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m           verbose=VERBOSE, validation_split=VALIDATION_SPLIT, shuffle= True)\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/deep-learning/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/envs/deep-learning/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep-learning/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2721\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep-learning/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2693\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep-learning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep-learning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep-learning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep-learning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep-learning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_dim = 3\n",
    "timesteps = 1\n",
    "num_classes = 7\n",
    "\n",
    "#RNN Setting\n",
    "NB_EPOCH = 15\n",
    "\n",
    "#The batch size\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "#In order to display the results\n",
    "VERBOSE = 1\n",
    "\n",
    "# the validation split used during the validation process represent 20% of the training data set\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "\n",
    "# expected input data shape: (batch_size, timesteps, data_dim) , return_sequences=True\n",
    "model = Sequential()\n",
    "\n",
    "#Using an LSTML RNN hidden layer which is very recommended in HAR deeplearning processing \n",
    "#the LSTM model to overcome the vanishing gradient problem that occurs with most Recurrent Neural Network models\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu', input_shape=(timesteps, data_dim)))  \n",
    "\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "\n",
    "#This layer is used to resize the output from 3D to 2D \n",
    "model.add(Flatten())\n",
    "\n",
    "#We use a dense layer in order to interpret the results\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#RMSprop This optimizer is usually a good choice for recurrent neural networks.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "#we display the summary of our model\n",
    "model.summary()\n",
    "\n",
    "#One hot encoding the categorical classes:\n",
    "#One hot encoding is a process by which categorical variables are converted into a form that could be provided \n",
    "#to ML algorithms to do a better job in prediction.\n",
    "yy_train = to_categorical(y_train,num_classes=num_classes)\n",
    "\n",
    "#We train our training model with the specefied setting\n",
    "model.fit(x_train, yy_train,\n",
    "          batch_size=BATCH_SIZE, epochs= NB_EPOCH,\n",
    "          verbose=VERBOSE, validation_split=VALIDATION_SPLIT, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384636/384636 [==============================] - 41s 106us/step\n",
      "Test score: 0.7996113908314542\n",
      "Test accuracy: 0.7314265955357191\n"
     ]
    }
   ],
   "source": [
    "#One hot encoding the categorical classes\n",
    "yy_test = to_categorical(y_test,num_classes=num_classes)\n",
    "\n",
    "score = model.evaluate(x_test, yy_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n"
     ]
    }
   ],
   "source": [
    "#display the loss and the accurac\n",
    "print(model.metrics_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
